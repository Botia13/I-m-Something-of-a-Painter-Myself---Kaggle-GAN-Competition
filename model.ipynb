{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cdea4b78",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988d67ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPU(s)\n",
      "GPU memory growth enabled\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Using MirroredStrategy with 1 replica(s)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"Found {len(gpus)} GPU(s)\")\n",
    "\n",
    "if gpus:\n",
    "    # Enable memory growth to avoid allocating all GPU memory at once\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    # Use MirroredStrategy for single or multiple GPUs\n",
    "    # It works efficiently with 1 GPU and scales to multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} replica(s)\")\n",
    "else:\n",
    "    # Fallback to CPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path = r\"C:\\Users\\david\\OneDrive\\Documentos\\Data Science\\Projects\\I'm something of a painter myself - GANs\\gan-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b59539",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "IMAGE_SIZE = [256, 256]\n",
    "\n",
    "monet_filenames = tf.io.gfile.glob(gcs_path + '/monet_tfrec/*.tfrec')\n",
    "photo_filenames = tf.io.gfile.glob(gcs_path + '/photo_tfrec/*.tfrec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5687a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = {'image': tf.io.FixedLenFeature([], tf.string)}\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    image_data = tf.io.parse_single_example(example, features)\n",
    "    image = image_data['image']\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "\n",
    "monet_dataset = tf.data.TFRecordDataset(monet_filenames).map(read_tfrecord, num_parallel_calls=AUTOTUNE).batch(1)\n",
    "photo_dataset = tf.data.TFRecordDataset(photo_filenames).map(read_tfrecord, num_parallel_calls=AUTOTUNE).batch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e2ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 7038\n"
     ]
    }
   ],
   "source": [
    "monet_dataset_len = len(list(iter(monet_dataset)))\n",
    "photo_dataset_len = len(list(photo_dataset.as_numpy_iterator()))\n",
    "print(monet_dataset_len, photo_dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a935f904",
   "metadata": {},
   "outputs": [],
   "source": [
    "example = next(iter(monet_dataset))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9f042b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "\n",
    "class CycleGan(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CycleGan, self).__init__()\n",
    "        self.monet_generator = self.create_generator()\n",
    "        self.photo_generator = self.create_generator()\n",
    "        self.monet_discriminator = self.create_discriminator()\n",
    "        self.photo_discriminator = self.create_discriminator()\n",
    "        self.lambda_cycle = 10\n",
    "\n",
    "    def compile(self):\n",
    "        super(CycleGan, self).compile()\n",
    "        self.monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    def create_downsampler(self, filters, size, apply_instance_norm=True):\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            layers.Conv2D(\n",
    "                filters,\n",
    "                size,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "                kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            )\n",
    "        )\n",
    "        if apply_instance_norm:\n",
    "            model.add(\n",
    "                tfa.layers.InstanceNormalization(\n",
    "                    gamma_initializer=keras.initializers.RandomNormal(\n",
    "                        mean=0.0, stddev=0.02\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        model.add(layers.LeakyReLU())\n",
    "        return model\n",
    "\n",
    "    def create_upsampler(self, filters, size, apply_dropout=False):\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            layers.Conv2DTranspose(\n",
    "                filters,\n",
    "                size,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "                kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            tfa.layers.InstanceNormalization(\n",
    "                gamma_initializer=tf.random_normal_initializer(0.0, 0.02)\n",
    "            )\n",
    "        )\n",
    "        if apply_dropout:\n",
    "            model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.ReLU())\n",
    "        return model\n",
    "\n",
    "    def create_generator(self):\n",
    "        downsampler_stack = [\n",
    "            self.create_downsampler(64, 4, apply_instance_norm=False),\n",
    "            self.create_downsampler(128, 4),\n",
    "            self.create_downsampler(256, 4),\n",
    "        ] + [self.create_downsampler(512, 4) for i in range(5)]\n",
    "        upsampler_stack = [\n",
    "            self.create_upsampler(512, 4, apply_dropout=True) for i in range(3)\n",
    "        ] + [\n",
    "            self.create_upsampler(512, 4),\n",
    "            self.create_upsampler(256, 4),\n",
    "            self.create_upsampler(128, 4),\n",
    "            self.create_upsampler(64, 4),\n",
    "        ]\n",
    "        input_layer = layers.Input(shape=[256, 256, 3])\n",
    "        x = input_layer\n",
    "        skips = []\n",
    "        for downsampler in downsampler_stack:\n",
    "            x = downsampler(x)\n",
    "            skips.append(x)\n",
    "        skips = reversed(skips[:-1])\n",
    "        for upsampler, skip_layer in zip(upsampler_stack, skips):\n",
    "            x = upsampler(x)\n",
    "            x = layers.Concatenate()([x, skip_layer])\n",
    "        last_layer = layers.Conv2DTranspose(\n",
    "            OUTPUT_CHANNELS,\n",
    "            4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "        x = last_layer(x)\n",
    "        return keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    def create_discriminator(self):\n",
    "        input_layer = layers.Input(shape=[256, 256, 3], name=\"input_image\")\n",
    "        x = input_layer\n",
    "        downsampler1 = self.create_downsampler(64, 4, False)(x)\n",
    "        downsampler2 = self.create_downsampler(128, 4)(downsampler1)\n",
    "        downsampler3 = self.create_downsampler(256, 4)(downsampler2)\n",
    "        zero_pad1 = layers.ZeroPadding2D()(downsampler3)\n",
    "        conv_layer = layers.Conv2D(\n",
    "            512,\n",
    "            4,\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "        )(zero_pad1)\n",
    "        normalization_layer1 = tfa.layers.InstanceNormalization(\n",
    "            gamma_initializer=tf.random_normal_initializer(0.0, 0.02)\n",
    "        )(conv_layer)\n",
    "        leaky_relu_layer = layers.LeakyReLU()(normalization_layer1)\n",
    "        zero_pad2 = layers.ZeroPadding2D()(leaky_relu_layer)\n",
    "        last_layer = layers.Conv2D(\n",
    "            1, 4, strides=1, kernel_initializer=tf.random_normal_initializer(0.0, 0.02)\n",
    "        )(zero_pad2)\n",
    "        return tf.keras.Model(inputs=input_layer, outputs=last_layer)\n",
    "\n",
    "    def discriminator_loss_fn(self, real, fake):\n",
    "        real_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )(tf.ones_like(real), real)\n",
    "        fake_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )(tf.zeros_like(fake), fake)\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def generator_loss_fn(self, generated_image):\n",
    "        return tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )(tf.ones_like(generated_image), generated_image)\n",
    "\n",
    "    def cycle_loss_fn(self, image, cycled_image, lambda_cycle):\n",
    "        return tf.reduce_mean(tf.abs(image - cycled_image)) * lambda_cycle\n",
    "\n",
    "    def identity_loss_fn(self, real_photo, photo, lambda_cycle):\n",
    "        return tf.reduce_mean(tf.abs(real_photo - photo)) * lambda_cycle / 2\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        real_monet, real_photo = batch_data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            fake_monet = self.monet_generator(real_photo, training=True)\n",
    "            cycled_photo = self.photo_generator(fake_monet, training=True)\n",
    "            fake_photo = self.photo_generator(real_monet, training=True)\n",
    "            cycled_monet = self.monet_generator(fake_photo, training=True)\n",
    "\n",
    "            monet1 = self.monet_generator(real_monet, training=True)\n",
    "            photo1 = self.photo_generator(real_photo, training=True)\n",
    "\n",
    "            monet_real_discriminated = self.monet_discriminator(\n",
    "                real_monet, training=True\n",
    "            )\n",
    "            monet_fake_discriminated = self.monet_discriminator(\n",
    "                fake_monet, training=True\n",
    "            )\n",
    "            photo_real_discriminated = self.photo_discriminator(\n",
    "                real_photo, training=True\n",
    "            )\n",
    "            photo_fake_discriminated = self.photo_discriminator(\n",
    "                fake_photo, training=True\n",
    "            )\n",
    "\n",
    "            monet_generator_loss = self.generator_loss_fn(monet_fake_discriminated)\n",
    "            photo_generator_loss = self.generator_loss_fn(photo_fake_discriminated)\n",
    "            cycle_loss = self.cycle_loss_fn(\n",
    "                real_monet, cycled_monet, self.lambda_cycle\n",
    "            ) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
    "            total_monet_generator_loss = (\n",
    "                monet_generator_loss\n",
    "                + cycle_loss\n",
    "                + self.identity_loss_fn(real_monet, monet1, self.lambda_cycle)\n",
    "            )\n",
    "            total_photo_generator_loss = (\n",
    "                photo_generator_loss\n",
    "                + cycle_loss\n",
    "                + self.identity_loss_fn(real_photo, photo1, self.lambda_cycle)\n",
    "            )\n",
    "            monet_discriminator_loss = self.discriminator_loss_fn(\n",
    "                monet_real_discriminated, monet_fake_discriminated\n",
    "            )\n",
    "            photo_discriminator_loss = self.discriminator_loss_fn(\n",
    "                photo_real_discriminated, photo_fake_discriminated\n",
    "            )\n",
    "        monet_generator_gradients = tape.gradient(\n",
    "            total_monet_generator_loss, self.monet_generator.trainable_variables\n",
    "        )\n",
    "        photo_generator_gradients = tape.gradient(\n",
    "            total_photo_generator_loss, self.photo_generator.trainable_variables\n",
    "        )\n",
    "        monet_discriminator_gradients = tape.gradient(\n",
    "            monet_discriminator_loss, self.monet_discriminator.trainable_variables\n",
    "        )\n",
    "        photo_discriminator_gradients = tape.gradient(\n",
    "            photo_discriminator_loss, self.photo_discriminator.trainable_variables\n",
    "        )\n",
    "        self.monet_generator_optimizer.apply_gradients(\n",
    "            zip(monet_generator_gradients, self.monet_generator.trainable_variables)\n",
    "        )\n",
    "        self.photo_generator_optimizer.apply_gradients(\n",
    "            zip(photo_generator_gradients, self.photo_generator.trainable_variables)\n",
    "        )\n",
    "        self.monet_discriminator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                monet_discriminator_gradients,\n",
    "                self.monet_discriminator.trainable_variables,\n",
    "            )\n",
    "        )\n",
    "        self.photo_discriminator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                photo_discriminator_gradients,\n",
    "                self.photo_discriminator.trainable_variables,\n",
    "            )\n",
    "        )\n",
    "        return {\n",
    "            \"monet_generator_loss\": total_monet_generator_loss,\n",
    "            \"photo_generator_loss\": total_photo_generator_loss,\n",
    "            \"monet_discriminator_loss\": monet_discriminator_loss,\n",
    "            \"photo_discriminator_loss\": photo_discriminator_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2239bc72",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGan()\n",
    "    cycle_gan_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e981cbb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "300/300 [==============================] - 82s 191ms/step - monet_generator_loss: 5.2056 - photo_generator_loss: 5.4490 - monet_discriminator_loss: 0.6435 - photo_discriminator_loss: 0.5993\n",
      "Epoch 2/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 3.6703 - photo_generator_loss: 3.7487 - monet_discriminator_loss: 0.6397 - photo_discriminator_loss: 0.6068\n",
      "Epoch 3/30\n",
      "300/300 [==============================] - 57s 192ms/step - monet_generator_loss: 3.6785 - photo_generator_loss: 3.8190 - monet_discriminator_loss: 0.6381 - photo_discriminator_loss: 0.5960\n",
      "Epoch 4/30\n",
      "300/300 [==============================] - 58s 192ms/step - monet_generator_loss: 3.5583 - photo_generator_loss: 3.7669 - monet_discriminator_loss: 0.6396 - photo_discriminator_loss: 0.5745\n",
      "Epoch 5/30\n",
      "300/300 [==============================] - 58s 192ms/step - monet_generator_loss: 3.3764 - photo_generator_loss: 3.5523 - monet_discriminator_loss: 0.6382 - photo_discriminator_loss: 0.6045\n",
      "Epoch 6/30\n",
      "300/300 [==============================] - 58s 192ms/step - monet_generator_loss: 3.2924 - photo_generator_loss: 3.4441 - monet_discriminator_loss: 0.6361 - photo_discriminator_loss: 0.6174\n",
      "Epoch 7/30\n",
      "300/300 [==============================] - 58s 192ms/step - monet_generator_loss: 3.2677 - photo_generator_loss: 3.3763 - monet_discriminator_loss: 0.6129 - photo_discriminator_loss: 0.6099\n",
      "Epoch 8/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 3.2396 - photo_generator_loss: 3.2856 - monet_discriminator_loss: 0.6138 - photo_discriminator_loss: 0.6184\n",
      "Epoch 9/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 3.1761 - photo_generator_loss: 3.2056 - monet_discriminator_loss: 0.6121 - photo_discriminator_loss: 0.6186\n",
      "Epoch 10/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 3.1524 - photo_generator_loss: 3.1642 - monet_discriminator_loss: 0.6105 - photo_discriminator_loss: 0.6205\n",
      "Epoch 11/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 3.0578 - photo_generator_loss: 3.1116 - monet_discriminator_loss: 0.6218 - photo_discriminator_loss: 0.6257\n",
      "Epoch 12/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.9707 - photo_generator_loss: 3.0077 - monet_discriminator_loss: 0.6205 - photo_discriminator_loss: 0.6243\n",
      "Epoch 13/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 3.0641 - photo_generator_loss: 3.1040 - monet_discriminator_loss: 0.5957 - photo_discriminator_loss: 0.6021\n",
      "Epoch 14/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.9887 - photo_generator_loss: 3.0480 - monet_discriminator_loss: 0.6073 - photo_discriminator_loss: 0.6137\n",
      "Epoch 15/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.9048 - photo_generator_loss: 2.9776 - monet_discriminator_loss: 0.6146 - photo_discriminator_loss: 0.6149\n",
      "Epoch 16/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.8791 - photo_generator_loss: 2.9535 - monet_discriminator_loss: 0.6163 - photo_discriminator_loss: 0.6132\n",
      "Epoch 17/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.8034 - photo_generator_loss: 2.8669 - monet_discriminator_loss: 0.6226 - photo_discriminator_loss: 0.6205\n",
      "Epoch 18/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.8142 - photo_generator_loss: 2.8898 - monet_discriminator_loss: 0.6177 - photo_discriminator_loss: 0.6173\n",
      "Epoch 19/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.7530 - photo_generator_loss: 2.8081 - monet_discriminator_loss: 0.6211 - photo_discriminator_loss: 0.6166\n",
      "Epoch 20/30\n",
      "300/300 [==============================] - 58s 193ms/step - monet_generator_loss: 2.7750 - photo_generator_loss: 2.8295 - monet_discriminator_loss: 0.6127 - photo_discriminator_loss: 0.6190\n",
      "Epoch 21/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.7464 - photo_generator_loss: 2.8376 - monet_discriminator_loss: 0.6186 - photo_discriminator_loss: 0.6068\n",
      "Epoch 22/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.6508 - photo_generator_loss: 2.7461 - monet_discriminator_loss: 0.6346 - photo_discriminator_loss: 0.6283\n",
      "Epoch 23/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.5919 - photo_generator_loss: 2.6855 - monet_discriminator_loss: 0.6385 - photo_discriminator_loss: 0.6316\n",
      "Epoch 24/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.6824 - photo_generator_loss: 2.7893 - monet_discriminator_loss: 0.6236 - photo_discriminator_loss: 0.6158\n",
      "Epoch 25/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.5998 - photo_generator_loss: 2.7201 - monet_discriminator_loss: 0.6334 - photo_discriminator_loss: 0.6200\n",
      "Epoch 26/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.6309 - photo_generator_loss: 2.7345 - monet_discriminator_loss: 0.6209 - photo_discriminator_loss: 0.6116\n",
      "Epoch 27/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.6157 - photo_generator_loss: 2.7312 - monet_discriminator_loss: 0.6332 - photo_discriminator_loss: 0.6191\n",
      "Epoch 28/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.5970 - photo_generator_loss: 2.7405 - monet_discriminator_loss: 0.6335 - photo_discriminator_loss: 0.6062\n",
      "Epoch 29/30\n",
      "300/300 [==============================] - 57s 190ms/step - monet_generator_loss: 2.6232 - photo_generator_loss: 2.7266 - monet_discriminator_loss: 0.6256 - photo_discriminator_loss: 0.6025\n",
      "Epoch 30/30\n",
      "300/300 [==============================] - 57s 191ms/step - monet_generator_loss: 2.5779 - photo_generator_loss: 2.7387 - monet_discriminator_loss: 0.6349 - photo_discriminator_loss: 0.6072\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x18a46499a30>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((monet_dataset.repeat(-1), photo_dataset.repeat(-1))),\n",
    "    steps_per_epoch=300,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('images.zip', mode='w') as zip_file:\n",
    "    i = 1\n",
    "    for img in photo_dataset:\n",
    "        generated_image_data = cycle_gan_model.monet_generator(img, training=False)[0].numpy()\n",
    "        scaled_generated_image_data = (generated_image_data * 127.5 + 127.5).astype(np.uint8)\n",
    "        with BytesIO() as image_bytes_io:\n",
    "            Image.fromarray(scaled_generated_image_data).save(image_bytes_io, 'JPEG')\n",
    "            image_bytes_io.seek(0)\n",
    "            zip_file.writestr('{}.jpg'.format(i), image_bytes_io.read())\n",
    "            i += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
