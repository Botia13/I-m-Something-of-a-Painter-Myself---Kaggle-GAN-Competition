{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cdea4b78",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\david\\miniconda3\\envs\\tf-final\\lib\\site-packages\\tensorflow_addons\\utils\\tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "c:\\Users\\david\\miniconda3\\envs\\tf-final\\lib\\site-packages\\tensorflow_addons\\utils\\ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.12.0 and strictly below 2.15.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.10.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "from zipfile import ZipFile\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12679176",
   "metadata": {},
   "source": [
    "# Problem Description"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ed92e6",
   "metadata": {},
   "source": [
    "The idea is to train a Generative Adversarial Network (GAN) that can turn regular photos into images that look like paintings. Along the way, we explain in simple terms how generative models create new data and how GANs use a generator and a discriminator to learn from each other. By the end, we aim to have a working model and a good sense of how these techniques actually come together."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35f4c0c9",
   "metadata": {},
   "source": [
    "### Checking for the GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "988d67ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 GPU(s)\n",
      "GPU memory growth enabled\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0',)\n",
      "Using MirroredStrategy with 1 replica(s)\n"
     ]
    }
   ],
   "source": [
    "# Check for GPUs\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "print(f\"Found {len(gpus)} GPU(s)\")\n",
    "\n",
    "if gpus:\n",
    "    # Enable memory growth to avoid allocating all GPU memory at once\n",
    "    try:\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "        print(\"GPU memory growth enabled\")\n",
    "    except RuntimeError as e:\n",
    "        print(e)\n",
    "    \n",
    "    # Use MirroredStrategy for single or multiple GPUs\n",
    "    # It works efficiently with 1 GPU and scales to multiple GPUs\n",
    "    strategy = tf.distribute.MirroredStrategy()\n",
    "    print(f\"Using MirroredStrategy with {strategy.num_replicas_in_sync} replica(s)\")\n",
    "else:\n",
    "    # Fallback to CPU\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "    print(\"No GPU found, using CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb001943",
   "metadata": {},
   "source": [
    "# Downloading the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9c044a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gcs_path = r\"C:\\Users\\david\\OneDrive\\Documentos\\Data Science\\Projects\\I'm something of a painter myself - GANs\\gan-getting-started\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "63b59539",
   "metadata": {},
   "outputs": [],
   "source": [
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "IMAGE_SIZE = [256, 256]\n",
    "\n",
    "monet_filenames = tf.io.gfile.glob(gcs_path + '/monet_tfrec/*.tfrec')\n",
    "photo_filenames = tf.io.gfile.glob(gcs_path + '/photo_tfrec/*.tfrec')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384df17",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5687a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the feature description for TFRecord parsing: we have only the image feature, stored as raw bytes\n",
    "features = {'image': tf.io.FixedLenFeature([], tf.string)}\n",
    "\n",
    "def read_tfrecord(example):\n",
    "    # Parse the input tf.train.Example proto using the feature description\n",
    "    image_data = tf.io.parse_single_example(example, features)\n",
    "    # Extract the raw image bytes\n",
    "    image = image_data['image']\n",
    "    # Decode the JPEG-encoded image into a tensor\n",
    "    image = tf.image.decode_jpeg(image, channels=3)\n",
    "    # Scale pixel values from [0, 255] to [-1, 1]\n",
    "    image = (tf.cast(image, tf.float32) / 127.5) - 1\n",
    "    # Reshape the image tensor to the required dimensions\n",
    "    image = tf.reshape(image, [*IMAGE_SIZE, 3])\n",
    "    return image\n",
    "\n",
    "# Create the Monet dataset by reading TFRecord files, parsing them, and batching.\n",
    "monet_dataset = tf.data.TFRecordDataset(monet_filenames)\\\n",
    "    .map(read_tfrecord, num_parallel_calls=AUTOTUNE)\\\n",
    "    .batch(1)\n",
    "\n",
    "# Create the Photo dataset in the same fashion.\n",
    "photo_dataset = tf.data.TFRecordDataset(photo_filenames)\\\n",
    "    .map(read_tfrecord, num_parallel_calls=AUTOTUNE)\\\n",
    "    .batch(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba45ea6",
   "metadata": {},
   "source": [
    "### Data Description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "89e2ff46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 7038\n"
     ]
    }
   ],
   "source": [
    "# Calculate the number of elements in the Monet dataset by converting it to a list and getting its length\n",
    "monet_dataset_len = len(list(iter(monet_dataset)))\n",
    "# Calculate the number of elements in the Photo dataset using as_numpy_iterator for iteration\n",
    "photo_dataset_len = len(list(photo_dataset.as_numpy_iterator()))\n",
    "# Print the lengths of both datasets\n",
    "print(monet_dataset_len, photo_dataset_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a935f904",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 256, 256, 3), dtype=float32, numpy=\n",
       "array([[[[-0.69411767, -0.5529412 , -1.        ],\n",
       "         [-0.5058824 , -0.372549  , -0.8039216 ],\n",
       "         [-0.1372549 , -0.00392157, -0.35686272],\n",
       "         ...,\n",
       "         [-0.05882353,  0.09019613, -0.06666666],\n",
       "         [-0.14509803,  0.0196079 , -0.12941176],\n",
       "         [-0.08235294,  0.09803927, -0.05882353]],\n",
       "\n",
       "        [[-0.27843136, -0.3960784 , -0.85882354],\n",
       "         [-0.17647058, -0.24705881, -0.6784314 ],\n",
       "         [ 0.03529418,  0.00392163, -0.36470586],\n",
       "         ...,\n",
       "         [-0.01176471,  0.13725495,  0.09019613],\n",
       "         [-0.08235294,  0.082353  ,  0.02745104],\n",
       "         [-0.02745098,  0.15294123,  0.09019613]],\n",
       "\n",
       "        [[-0.11372548, -0.23137254, -0.7411765 ],\n",
       "         [-0.00392157, -0.09803921, -0.5686275 ],\n",
       "         [ 0.11372554,  0.05882359, -0.36470586],\n",
       "         ...,\n",
       "         [ 0.00392163,  0.18431377,  0.2313726 ],\n",
       "         [-0.05882353,  0.12941182,  0.16078436],\n",
       "         [-0.01960784,  0.1686275 ,  0.20000005]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.38039213, -0.29411763, -0.56078434],\n",
       "         [-0.46666664, -0.38039213, -0.64705884],\n",
       "         [-0.5372549 , -0.45098037, -0.7019608 ],\n",
       "         ...,\n",
       "         [-0.05882353,  0.07450986, -0.19215685],\n",
       "         [-0.08235294,  0.0196079 , -0.32549018],\n",
       "         [-0.21568626, -0.1372549 , -0.5529412 ]],\n",
       "\n",
       "        [[-0.60784316, -0.5294118 , -0.81960785],\n",
       "         [-0.6       , -0.52156866, -0.8117647 ],\n",
       "         [-0.5686275 , -0.5137255 , -0.78039217],\n",
       "         ...,\n",
       "         [ 0.05882359,  0.10588241, -0.27058822],\n",
       "         [-0.12156862, -0.09019607, -0.4823529 ],\n",
       "         [-0.24705881, -0.23137254, -0.62352943]],\n",
       "\n",
       "        [[-0.5529412 , -0.49019605, -0.79607844],\n",
       "         [-0.6       , -0.5372549 , -0.84313726],\n",
       "         [-0.5921569 , -0.54509807, -0.827451  ],\n",
       "         ...,\n",
       "         [ 0.20000005,  0.15294123, -0.23921567],\n",
       "         [-0.02745098, -0.05882353, -0.56078434],\n",
       "         [-0.26274508, -0.25490195, -0.8666667 ]]]], dtype=float32)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example = next(iter(monet_dataset))\n",
    "example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c92d0a",
   "metadata": {},
   "source": [
    "# Model Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9f042b90",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of output channels for the generator (e.g., RGB)\n",
    "OUTPUT_CHANNELS = 3\n",
    "\n",
    "class CycleGan(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CycleGan, self).__init__()\n",
    "        # Define the Monet and Photo generators\n",
    "        self.monet_generator = self.create_generator()\n",
    "        self.photo_generator = self.create_generator()\n",
    "        # Define the Monet and Photo discriminators\n",
    "        self.monet_discriminator = self.create_discriminator()\n",
    "        self.photo_discriminator = self.create_discriminator()\n",
    "        # Lambda for cycle consistency loss\n",
    "        self.lambda_cycle = 10\n",
    "\n",
    "    def compile(self):\n",
    "        # Compile method to instantiate separate optimizers for each model component\n",
    "        super(CycleGan, self).compile()\n",
    "        self.monet_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.monet_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.photo_generator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.photo_discriminator_optimizer = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "    def create_downsampler(self, filters, size, apply_instance_norm=True):\n",
    "        # Downsampling block for generators and discriminators\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            layers.Conv2D(\n",
    "                filters,\n",
    "                size,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "                kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            )\n",
    "        )\n",
    "        if apply_instance_norm:\n",
    "            model.add(\n",
    "                tfa.layers.InstanceNormalization(\n",
    "                    gamma_initializer=keras.initializers.RandomNormal(\n",
    "                        mean=0.0, stddev=0.02\n",
    "                    )\n",
    "                )\n",
    "            )\n",
    "        model.add(layers.LeakyReLU())\n",
    "        return model\n",
    "\n",
    "    def create_upsampler(self, filters, size, apply_dropout=False):\n",
    "        # Upsampling block for generators\n",
    "        model = keras.Sequential()\n",
    "        model.add(\n",
    "            layers.Conv2DTranspose(\n",
    "                filters,\n",
    "                size,\n",
    "                strides=2,\n",
    "                padding=\"same\",\n",
    "                use_bias=False,\n",
    "                kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            )\n",
    "        )\n",
    "        model.add(\n",
    "            tfa.layers.InstanceNormalization(\n",
    "                gamma_initializer=tf.random_normal_initializer(0.0, 0.02)\n",
    "            )\n",
    "        )\n",
    "        if apply_dropout:\n",
    "            model.add(layers.Dropout(0.5))\n",
    "        model.add(layers.ReLU())\n",
    "        return model\n",
    "\n",
    "    def create_generator(self):\n",
    "        # Construct the U-Net generator architecture\n",
    "        # Downsampling layers\n",
    "        downsampler_stack = [\n",
    "            self.create_downsampler(64, 4, apply_instance_norm=False),\n",
    "            self.create_downsampler(128, 4),\n",
    "            self.create_downsampler(256, 4),\n",
    "        ] + [self.create_downsampler(512, 4) for i in range(5)]\n",
    "        # Upsampling layers\n",
    "        upsampler_stack = [\n",
    "            self.create_upsampler(512, 4, apply_dropout=True) for i in range(3)\n",
    "        ] + [\n",
    "            self.create_upsampler(512, 4),\n",
    "            self.create_upsampler(256, 4),\n",
    "            self.create_upsampler(128, 4),\n",
    "            self.create_upsampler(64, 4),\n",
    "        ]\n",
    "        # Input layer\n",
    "        input_layer = layers.Input(shape=[256, 256, 3])\n",
    "        x = input_layer\n",
    "        skips = []\n",
    "        for downsampler in downsampler_stack:\n",
    "            # Downsample and store skip connections\n",
    "            x = downsampler(x)\n",
    "            skips.append(x)\n",
    "        # Reverse skip connections, excluding the last\n",
    "        skips = reversed(skips[:-1])\n",
    "        for upsampler, skip_layer in zip(upsampler_stack, skips):\n",
    "            # Upsample and concatenate with skip connection\n",
    "            x = upsampler(x)\n",
    "            x = layers.Concatenate()([x, skip_layer])\n",
    "        # Final transposed conv layer to get output image\n",
    "        last_layer = layers.Conv2DTranspose(\n",
    "            OUTPUT_CHANNELS,\n",
    "            4,\n",
    "            strides=2,\n",
    "            padding=\"same\",\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "            activation=\"tanh\",\n",
    "        )\n",
    "        x = last_layer(x)\n",
    "        return keras.Model(inputs=input_layer, outputs=x)\n",
    "\n",
    "    def create_discriminator(self):\n",
    "        # Construct the PatchGAN discriminator architecture\n",
    "        input_layer = layers.Input(shape=[256, 256, 3], name=\"input_image\")\n",
    "        x = input_layer\n",
    "        # Downsampling blocks (no normalization in first block)\n",
    "        downsampler1 = self.create_downsampler(64, 4, False)(x)\n",
    "        downsampler2 = self.create_downsampler(128, 4)(downsampler1)\n",
    "        downsampler3 = self.create_downsampler(256, 4)(downsampler2)\n",
    "        # Padding and main conv layer\n",
    "        zero_pad1 = layers.ZeroPadding2D()(downsampler3)\n",
    "        conv_layer = layers.Conv2D(\n",
    "            512,\n",
    "            4,\n",
    "            strides=1,\n",
    "            use_bias=False,\n",
    "            kernel_initializer=tf.random_normal_initializer(0.0, 0.02),\n",
    "        )(zero_pad1)\n",
    "        normalization_layer1 = tfa.layers.InstanceNormalization(\n",
    "            gamma_initializer=tf.random_normal_initializer(0.0, 0.02)\n",
    "        )(conv_layer)\n",
    "        leaky_relu_layer = layers.LeakyReLU()(normalization_layer1)\n",
    "        zero_pad2 = layers.ZeroPadding2D()(leaky_relu_layer)\n",
    "        # Output layer for patchwise discrimination\n",
    "        last_layer = layers.Conv2D(\n",
    "            1, 4, strides=1, kernel_initializer=tf.random_normal_initializer(0.0, 0.02)\n",
    "        )(zero_pad2)\n",
    "        return tf.keras.Model(inputs=input_layer, outputs=last_layer)\n",
    "\n",
    "    def discriminator_loss_fn(self, real, fake):\n",
    "        # PatchGAN discriminator loss: compares real and fake with binary cross-entropy\n",
    "        real_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )(tf.ones_like(real), real)\n",
    "        fake_loss = tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )(tf.zeros_like(fake), fake)\n",
    "        return (real_loss + fake_loss) / 2\n",
    "\n",
    "    def generator_loss_fn(self, generated_image):\n",
    "        # Generator loss: how well generator fools discriminator (wants discriminator to output ones)\n",
    "        return tf.keras.losses.BinaryCrossentropy(\n",
    "            from_logits=True, reduction=tf.keras.losses.Reduction.NONE\n",
    "        )(tf.ones_like(generated_image), generated_image)\n",
    "\n",
    "    def cycle_loss_fn(self, image, cycled_image, lambda_cycle):\n",
    "        # Cycle consistency loss: difference between original and cycled image\n",
    "        return tf.reduce_mean(tf.abs(image - cycled_image)) * lambda_cycle\n",
    "\n",
    "    def identity_loss_fn(self, real_photo, photo, lambda_cycle):\n",
    "        # Identity loss: difference between real image and image after identity mapping through generator\n",
    "        return tf.reduce_mean(tf.abs(real_photo - photo)) * lambda_cycle / 2\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # Custom training step for CycleGAN\n",
    "        real_monet, real_photo = batch_data\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Forward pass: generate fakes and cycled images\n",
    "            fake_monet = self.monet_generator(real_photo, training=True)\n",
    "            cycled_photo = self.photo_generator(fake_monet, training=True)\n",
    "            fake_photo = self.photo_generator(real_monet, training=True)\n",
    "            cycled_monet = self.monet_generator(fake_photo, training=True)\n",
    "\n",
    "            # Identity mapping for identity loss\n",
    "            monet1 = self.monet_generator(real_monet, training=True)\n",
    "            photo1 = self.photo_generator(real_photo, training=True)\n",
    "\n",
    "            # Discriminator predictions for real and fake images\n",
    "            monet_real_discriminated = self.monet_discriminator(\n",
    "                real_monet, training=True\n",
    "            )\n",
    "            monet_fake_discriminated = self.monet_discriminator(\n",
    "                fake_monet, training=True\n",
    "            )\n",
    "            photo_real_discriminated = self.photo_discriminator(\n",
    "                real_photo, training=True\n",
    "            )\n",
    "            photo_fake_discriminated = self.photo_discriminator(\n",
    "                fake_photo, training=True\n",
    "            )\n",
    "\n",
    "            # Generator losses\n",
    "            monet_generator_loss = self.generator_loss_fn(monet_fake_discriminated)\n",
    "            photo_generator_loss = self.generator_loss_fn(photo_fake_discriminated)\n",
    "            # Cycle-consistency loss (sum for both directions)\n",
    "            cycle_loss = self.cycle_loss_fn(\n",
    "                real_monet, cycled_monet, self.lambda_cycle\n",
    "            ) + self.cycle_loss_fn(real_photo, cycled_photo, self.lambda_cycle)\n",
    "            # Total generator losses (GAN + cycle + identity)\n",
    "            total_monet_generator_loss = (\n",
    "                monet_generator_loss\n",
    "                + cycle_loss\n",
    "                + self.identity_loss_fn(real_monet, monet1, self.lambda_cycle)\n",
    "            )\n",
    "            total_photo_generator_loss = (\n",
    "                photo_generator_loss\n",
    "                + cycle_loss\n",
    "                + self.identity_loss_fn(real_photo, photo1, self.lambda_cycle)\n",
    "            )\n",
    "            # Discriminator losses\n",
    "            monet_discriminator_loss = self.discriminator_loss_fn(\n",
    "                monet_real_discriminated, monet_fake_discriminated\n",
    "            )\n",
    "            photo_discriminator_loss = self.discriminator_loss_fn(\n",
    "                photo_real_discriminated, photo_fake_discriminated\n",
    "            )\n",
    "        # Calculate gradients and apply them for all generator/discriminator networks\n",
    "        monet_generator_gradients = tape.gradient(\n",
    "            total_monet_generator_loss, self.monet_generator.trainable_variables\n",
    "        )\n",
    "        photo_generator_gradients = tape.gradient(\n",
    "            total_photo_generator_loss, self.photo_generator.trainable_variables\n",
    "        )\n",
    "        monet_discriminator_gradients = tape.gradient(\n",
    "            monet_discriminator_loss, self.monet_discriminator.trainable_variables\n",
    "        )\n",
    "        photo_discriminator_gradients = tape.gradient(\n",
    "            photo_discriminator_loss, self.photo_discriminator.trainable_variables\n",
    "        )\n",
    "        self.monet_generator_optimizer.apply_gradients(\n",
    "            zip(monet_generator_gradients, self.monet_generator.trainable_variables)\n",
    "        )\n",
    "        self.photo_generator_optimizer.apply_gradients(\n",
    "            zip(photo_generator_gradients, self.photo_generator.trainable_variables)\n",
    "        )\n",
    "        self.monet_discriminator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                monet_discriminator_gradients,\n",
    "                self.monet_discriminator.trainable_variables,\n",
    "            )\n",
    "        )\n",
    "        self.photo_discriminator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                photo_discriminator_gradients,\n",
    "                self.photo_discriminator.trainable_variables,\n",
    "            )\n",
    "        )\n",
    "        # Return training losses for monitoring\n",
    "        return {\n",
    "            \"monet_generator_loss\": total_monet_generator_loss,\n",
    "            \"photo_generator_loss\": total_photo_generator_loss,\n",
    "            \"monet_discriminator_loss\": monet_discriminator_loss,\n",
    "            \"photo_discriminator_loss\": photo_discriminator_loss,\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d0d322d",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2239bc72",
   "metadata": {},
   "outputs": [],
   "source": [
    "with strategy.scope():\n",
    "    cycle_gan_model = CycleGan()\n",
    "    cycle_gan_model.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e981cbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cycle_gan_model.fit(\n",
    "    tf.data.Dataset.zip((monet_dataset.repeat(-1), photo_dataset.repeat(-1))),\n",
    "    steps_per_epoch=300,\n",
    "    epochs=30\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a437e0aa",
   "metadata": {},
   "source": [
    "# Kaggle Delivery"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17b3be42",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ZipFile('images.zip', mode='w') as zip_file:\n",
    "    i = 1\n",
    "    for img in photo_dataset:\n",
    "        generated_image_data = cycle_gan_model.monet_generator(img, training=False)[0].numpy()\n",
    "        scaled_generated_image_data = (generated_image_data * 127.5 + 127.5).astype(np.uint8)\n",
    "        with BytesIO() as image_bytes_io:\n",
    "            Image.fromarray(scaled_generated_image_data).save(image_bytes_io, 'JPEG')\n",
    "            image_bytes_io.seek(0)\n",
    "            zip_file.writestr('{}.jpg'.format(i), image_bytes_io.read())\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1877bf2f",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bccd2f8",
   "metadata": {},
   "source": [
    "In this notebook, we successfully trained a CycleGAN model to translate photos into Monet-style paintings. We prepared the datasets, built and compiled the model using TensorFlow and Keras, and carried out the training process. Finally, we generated Monet-like images from the photo dataset and saved the results in a zipped file suitable for submission. This workflow demonstrates the powerful capabilities of generative models for artistic style transfer and reinforces the utility of deep learning in tackling creative tasks.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-final",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
